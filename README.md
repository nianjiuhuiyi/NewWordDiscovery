​		建议使用Typora查看本文档，并在左上角`文件(F)`中打开`偏好设置`；再在里面的`外观`中点击`打开主题文件夹`，之后把项目中的`drake.css`放在前面打开的文件夹内，最后再关闭本文档，重新打开，在`主题(T)`中选择`Drake`以获取更佳的阅读体验。

## 一、数据准备：

- `./data/original_data`：OCR得到的结果，先是做了一个正则匹配，去掉乱码，然后每个做了简单的人工过滤(正则匹配很简单，后续的人工筛选也是没办法脚本化，这就自己来)；

## 二、使用

每一个新词发现方法得到的结果，保留的都是与jieba分词得到的结果去重后剩下的。

1. `python 新词发现1.py`   # 这里它原本核心实现方法对一个文件处理都用了多进程，在window下使用可以，但是linux会报错，还没处理。所以建议把数据搞好，==在window下执行这==，（这运行速度很快），然后再整体打包，上服务器跑剩余的两个py文件。

   1. 使用的数据在文件夹`./data/original_data`，全是txt文档;

   2. 最终结果在文件夹`./results/result_1`中；

      - 这两个路径，可以在def main() 函数下几行进行修改

      >Ps：整个运行过程中会产生一些中间文件，最终在程序运行完后就会自动删除，可在`新词发现1.py`最后几行决定是否保留；具体如下：
      >
      >- `./result` 是此方法得到的csv结果，里面有每个词的词频统计、自由度、凝固度等，(一般用不到，想看一下这些属性时可以保留)
      >- `./temp`是此方法过程产生的中间文件，没意义，可直接删除
      >
      >在`新词发现1.py`的最后几行，可通过对列表的修改，选择是否保留。
      >
      >​		要修改新词发现的核心参数，n_gram，自由度这些，可以结合下面的参数说明，在`新词发现1.py`中找到与`demo.py`相同的函数，去直接修改参数就行。

      >​		一定要先把这运行完，比较快；会产生根据词性进行了筛选的结果，保留在`./data/procrssing_data`，特别注意，后续的两个方法使用的原始数据就是这个文件夹了，一定要注意。

2. `python 新词发现2.py`           # 这个运行时间较长

   1. 使用的数据在文件夹`./data/procrssing_data`,(上面有解释)
   2. 最终结果在`./results/result_2`

3. `python 新词发现3.py`   # 这个运行时间相当长

   1. 使用的数据在文件夹`./data/procrssing_data`,(上面有解释)

   2. 最终结果在`./results/result_3`

      >​		这个结果的一个说明（涉及到后续融合时结果的一个处理）：
      >
      >得到的新词txt文本中，是包含了所有的词的，这个方法对每个新词都有一个打分，得到的结果也是依据打分从高到低排列的(只是结果里没有体现)；
      >
      >​	这个太多了，越往后的结果可能不是那么好，建议可以根据最后结果新词总的个数，自己再选择保留一下，保留前百分之多少(比如前%5或%10)；当人也可以在源码中根据词的打分结果在第一次保留时就进行截取，但是那阈值不好定，文本不一样，差异挺大的。

4. `python merge.py`

   1. 使用的数据就是前面3个方法得到的数据
   2. 把这3个方法得到的数据最终融合到一起，放在`./final_result`，这里面就是每本书的最终结果

>​		Ps：工程项目中还有名为`./compare`的文件夹，用作一个直接分词和加入了人工获取自定义词典后分词的一个效果对比；直接运行里面的`compare.py`。



## 三、其它说明

### 3.1 针对`新词发现1.py`

1. 要修改新词发现的核心参数，n_gram，自由度这些，可以结合下面的参数说明，在`新词发现1.py`中找到名为`new_word_discover`函数，去直接修改参数就行。
2. 最终结果的一个说明：最终的txt结果是对csv文件结果（结合上面看，这是存放在  ./result 中的，最后当做中间文件删除了）的一个处理，只保留了词，其它属性就没要了；简单来说，csv是先是2个字的词，再是3个字的词，再是4个字的词…(字数相同的是按某个属性排列的，记不太住了)；然后部分4个字、5个字的一行csv是存在这样`受电弓供电方式    供电方式` 两个的可能性都有。

此方法的思想：

基于`统计`的方式使用多进程进行新词发现。 通过计算每个字间共同出现的概率（凝固度），及其自由度判断其是否为新词。

```
核心思想：
    凝固度  p_xy / (p_x * p_y)
        若词X 词Y 一起出现的概率除以其各出现概率，值越大，说明词X 词Y 经常出现在一起的概率越高
    自由度  sum(-pi*log(pi))
        若词X 词Y 两侧出现的词越多越杂，即其两侧取词的自由度越高，词X 词Y 越独立，
        若有一侧自由度很低，则说明 词X 词Y 不是单独出现，可能为 XYZ 词中一部分

通过设定最低概率、最低凝固度、自由度 来限制搜索到的词语数量
p_min（最小概率或频数）、co_min（凝固度）、h_max（自由度） 越小查找到的词语数量越多，相对准确率也下降
```

[参考资料: 基于信息熵的无字典分词算法](http://www.cnblogs.com/bigdatafly/p/5014597.html)

```
若使用新的文本数据，请对应修改 get_corpus.py  下 get_word（） 函数中文本读取代码，返回文本数据迭代器

主要参数：

--file_name  训练文件名称

【常用设置 重要参数】
--n_gram  提取的新词长度  默认为5。 即超过5个字符的新词不再处理
--p_min  词出现的最小概率 （p_min = 3 整数则为频数， p_min = 0.00001 浮点数则为概率） 【dytpe: int, default 0.00001 】
--co_min  最小凝固度，只有超过最小凝固度才继续判断自由度并进入下一步搜索  【dytpe: int, default 100】
--h_min   最大自由度，若小于最大自由度，则认为词组不完整，为大词组中的一小部分 【dytpe: int, default 1.2】

--level_s  CMD界面窗口显示日志级别. 【默认为INFO】
--level_f  日志文件记录级别. 【默认为INFO】

--batch_len  批次计算的文本字符串长度 。【 字符串长度减少可降低占用内存，默认1000000个字符就进入统计计算】
--top_n  保存 top_n 个词组 参数设置越大，结果准确度越高，内存也增加, 在硬件配置允许的条件下应尽量调高 【默认 1000000】
```

>​																		*NewWordDiscovery*
>
>核心方法 `new_word_discover`中的所有参数：
>
> """ 
>file:       待切词的文件 【绝对路径或文件名，若为文件名则默认存储路径为 .\\NLP\\Data】
>f_data_col: 提取数据的列序号 默认为None 【整数 从 0 开始】
>f_txt_sep:  txt 文件的切分字符  默认为None 【 csv 文件忽然此参数】
>f_encoding: 默认为utf8  utf8 | gbk
>n_gram:     提取的新词长度  默认为5。 即超过5个字符的新词不再处理
>batch_len:  批次计算的文本字符串长度 。【 字符串长度减少可降低占用内存，默认100000个字符就进入统计计算】
>top_n:      保存 top_n 个词组 参数设置越大，结果准确度越高，内存也增加, 在硬件配置允许的条件下应尽量调高 【默认 1000000】
>p_min:      词出现的最小概率 （p_min = 3 整数则为频数， p_min = 0.0001 浮点数则为概率）【默认 0.0001】
>co_min:     最小凝固度，只有超过最小凝固度才继续判断自由度并进入下一步搜索  【dytpe: int, default 100】
>h_min:      最大自由度，若小于最大自由度，则认为词组不完整，为大词组中的一小部分  【dytpe: int, default 1.2】
>level_s:    界面显示日志级别.  ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']  默认 INFO
>level_f:    日志文件记录级别.  ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']  默认 INFO
>log_path:   日志存储路径，默认为 None，默认存储到 .\\NLP\\log\\NLP_[当前时间].log
>process_no: 多进程处理的进程数，int 类型，默认为None 即 CPU 核数
>"""

### 3.2 针对`新词发现3.py`

​										Chinese_segment_augment

​	python利用互信息和左右信息熵的中文分词新词发现

1. 简介

   - 使用[jieba分词](https://github.com/fxsjy/jieba)为基本分词组件
   - 针对用户给出的文本，利用信息熵进行新词发现
   - 使用[字典树](https://github.com/zhanzecheng/The-Art-Of-Programming-By-July/blob/master/ebook/zh/06.09.md)存储单词和统计词频
   - 由于但文本不能正确反映单个词的词频，这里使用[jieba](https://github.com/fxsjy/jieba)自带的词频表作为外部数据源
   - 取 TOP N 个作为新词

2. 方法解释

   - 先使用jieba分词对demo.txt做粗略分词

   - 使用 3 gram 的方式来构建节点，并使用词典树对存储分词，如

         [4G, 网络， 上网卡] --> [4G, 网络， 上网卡, 4G网络, 网络上网卡, 4G网络上网卡]

   - 利用trie树计算互信息 PMI

   - 利用trie树计算左右熵

   - 得出得分 score = PMI + min(左熵， 右熵)

   - 以得分高低进行排序，取出前5个，若前面的待选词在属于后面待选词一部分，则删除后面待选词，如

         [花呗， 蚂蚁花呗] --> [花呗]

   具体原理说明请看这个[链接](https://www.jianshu.com/p/e9313fd692ef)

